{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most real life data sets contain a certain amount of\n",
    "redundant data, which does not contribute\n",
    "significantly to the formation of important\n",
    "relationships. This redundancy not only increases\n",
    "the dimensionality of the data set and slows down\n",
    "the data mining process but also affects the\n",
    "subsequent classification performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atribute selection\n",
    "Attribute selection is the process of removing\n",
    "the redundant attributes that are deemed irrelevant\n",
    "to the data mining task. However, the presence of\n",
    "attributes that are not useful to classification might\n",
    "interfere with the relevant attributes to degrade\n",
    "classification performance. This is due to the noise\n",
    "that is contributed by these additional attributes and\n",
    "raises the level of difficult [21]\n",
    "The objective of attribute selection is therefore\n",
    "to search for a worthy set of attributes that produce\n",
    "comparable classification results to the case when\n",
    "all the attributes are used. In addition, a smaller set\n",
    "of attributes also creates less complicated patterns,\n",
    "which are easily comprehensible, and even\n",
    "visualized, by humans [21]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection \n",
    "Feature selection methods tend to identify the\n",
    "features most relevant for classification and can be\n",
    "broadly categorized as either subset selection\n",
    "methods or ranking methods. The former type\n",
    "returns a subset of the original set of features which\n",
    "are considered to be the most important for\n",
    "classification [24][26]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    " The pre-processing step is necessary to resolve\n",
    "several types of problems including noisy data,\n",
    "redundant data, missing data values, etc. The high\n",
    "quality data will lead to high quality results and\n",
    "reduced costs for data mining. Missing data should\n",
    "be pre-processed so as to allow the whole data set to\n",
    "be processed by a required algorithm. Moreover,\n",
    "most of the existing algorithms are able to extract\n",
    "knowledge from data set that store discrete features.\n",
    "If the features are continuous, the algorithms can be\n",
    "integrated to create discrete attributes [22]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over sampling\n",
    "There are a number of methods available to oversample a dataset used in a typical classification problem (using a classification algorithm to classify a set of images, given a labelled training set of images). The most common technique is known as SMOTE: Synthetic Minority Over-sampling Technique.[2] To illustrate how this technique works consider some training data which has s samples, and f features in the feature space of the data. Note that these features, for simplicity, are continuous. As an example, consider a dataset of birds for clarification. The feature space for the minority class for which we want to oversample could be beak length, wingspan, and weight (all continuous). To then oversample, take a sample from the dataset, and consider its k nearest neighbors (in feature space). To create a synthetic data point, take the vector between one of those k neighbors, and the current data point. Multiply this vector by a random number x which lies between 0, and 1. Add this to the current data point to create the new, synthetic data point. \n",
    "\n",
    "https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/chawla2002.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under sampling\n",
    "Cluster centroids is a method that replaces cluster of samples by the cluster centroid of a K-means algorithm, where the number of clusters is set by the level of undersampling.  \n",
    "https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis  \n",
    "https://sci2s.ugr.es/keel/pdf/specific/articulo/yen_cluster_2009.pdf 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features_resampled, target_resample, random_state=42)\n",
    "print('Training target statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing target statistics: {}'.format(Counter(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)\n",
    "clf_rf.fit(x_train, y_train)\n",
    "print('Validation Results')\n",
    "print(clf_rf.score(x_test, y_test))\n",
    "print(recall_score(y_test, clf_rf.predict(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
