{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from meassures import get_meassure\n",
    "from meassures import get_sensitive\n",
    "from meassures import get_specificity\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from test_data import local_test\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>on_antithyroid_medication</th>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <th>query_hyperthyroid</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>sick</th>\n",
       "      <th>tumor</th>\n",
       "      <th>lithium</th>\n",
       "      <th>...</th>\n",
       "      <th>T4U_measured</th>\n",
       "      <th>FTI_measured</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>TSH</th>\n",
       "      <th>T3</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>109.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   on_thyroxine  query_on_thyroxine  on_antithyroid_medication  \\\n",
       "0             0                   0                          0   \n",
       "1             0                   0                          0   \n",
       "2             0                   0                          0   \n",
       "3             0                   0                          0   \n",
       "4             0                   0                          0   \n",
       "\n",
       "   thyroid_surgery  query_hypothyroid  query_hyperthyroid  pregnant  sick  \\\n",
       "0                0                  0                   0         0     0   \n",
       "1                0                  0                   0         0     0   \n",
       "2                0                  0                   0         0     0   \n",
       "3                0                  0                   0         0     0   \n",
       "4                0                  0                   0         0     0   \n",
       "\n",
       "   tumor  lithium   ...     T4U_measured  FTI_measured  age  sex  TSH   T3  \\\n",
       "0      0        0   ...                1             1   80    1  1.4  0.8   \n",
       "1      0        0   ...                1             1   74    0  0.0  0.7   \n",
       "2      0        0   ...                1             1   32    0  1.4  1.1   \n",
       "3      0        0   ...                1             1   42    0  2.3  1.1   \n",
       "4      0        0   ...                1             1   89    1  0.8  0.8   \n",
       "\n",
       "     TT4   T4U    FTI  classes  \n",
       "0  105.0  0.88  120.0        1  \n",
       "1   98.0  0.81  121.0        1  \n",
       "2  121.0  1.11  109.0        1  \n",
       "3   93.0  0.73  127.0        1  \n",
       "4  111.0  0.68  165.0        1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se cargan los datos\n",
    "data = pd.read_csv('full_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se separan características de etiquetas\n",
    "features = data.iloc[:, 0:-1]\n",
    "target = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3152, 23)\n",
      "(3152,)\n",
      "2864\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "# Número de muestras\n",
    "print(features.shape)\n",
    "print(target.shape)\n",
    "print(len(data[data.classes == 0]))\n",
    "print(len(data[data.classes == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos en numpy array\n",
    "features = np.array(features)\n",
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo sin hacer sobre y sub muestreo, tomando 30% de prueba y 70% de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Entrenamiento: \n",
      "Validación: \n",
      "-------------------------------------\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.72, Especificidad: 0.983451536643026\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Normalizamos los datos de entrenamientohttp://localhost:8888/notebooks/ANN.ipynb#Modelo-sin-hacer-sobre-y-sub-muestreo\n",
    "features = StandardScaler().fit_transform(features)\n",
    "\n",
    "# Realiamos la partición del conjunto de datos para entrenamiento y pruebas\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=.3, random_state=42)\n",
    "\n",
    "# Definimos el modelo y lo entrenamos\n",
    "model = MLPClassifier(hidden_layer_sizes=(36, 36), activation='tanh', max_iter=500)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Realizamos predicciones\n",
    "y_est_train = model.predict(x_train)\n",
    "y_est_test = model.predict(x_test)\n",
    "\n",
    "# Medimos la exactitud de clasificación\n",
    "score1 = model.score(x_train, y_train)\n",
    "score2 = model.score(x_test, y_test)\n",
    "\n",
    "print('-------------------------------------')\n",
    "print('Entrenamiento: '.format(score1))\n",
    "print('Validación: '.format(score2))\n",
    "print('-------------------------------------')\n",
    "\n",
    "# Medimos Sensibilidad y Especificidad\n",
    "tp, fp, tn, fn = get_meassure(y_test, y_est_test)\n",
    "sensitive = get_sensitive(tp, fn)\n",
    "specificity = get_specificity(tn, fp)\n",
    "\n",
    "# Predecimos la muestra local\n",
    "print(model.predict(local_test))\n",
    "\n",
    "print('-------------------------------------')\n",
    "print('Sensibilidad: {0}, Especificidad: {1}'.format(sensitive, specificity))\n",
    "print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la tasa de aciertos que el modelo dice tener es alta, se puede observar que los datos están en general mal clasificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo con sobre y sub muestreo utilizando validación estratificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de los nuevos conjuntos de clases: Counter({0: 350, 1: 350})\n"
     ]
    }
   ],
   "source": [
    "# Sobre y sub muestreo\n",
    "\n",
    "# Se separan los datos en conjuntos por clase\n",
    "major_class = data[data.classes == 0]\n",
    "minor_class = data[data.classes == 1]\n",
    "\n",
    "# Se hace un sobremuestreo sobre la clase minoritaria\n",
    "minor_class_upsampled = resample(minor_class, replace=True, n_samples=350, random_state=42)\n",
    "\n",
    "# Se unen los conjuntos en un nuevo conjunto único de clases\n",
    "data_new = pd.concat([major_class, minor_class_upsampled])\n",
    "\n",
    "# Se separan características de etiquetas\n",
    "features = data_new.iloc[:, 0:-1]\n",
    "target = data_new.iloc[:, -1]\n",
    "\n",
    "# Se hace sub muestreo de conjunto mayoritario\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "features, target = rus.fit_resample(features, target)\n",
    "print('Tamaño de los nuevos conjuntos de clases: {0}'.format(Counter(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Función de activación: logistic\n",
      "Número de neuronas: 20\n",
      "-------------------------------------\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9149731606813152 +- 0.013399832138961225\n",
      "Validación: 0.8957289517634345 +- 0.011079326638343807\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.9051724137931034, Especificidad: 0.896551724137931\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: logistic\n",
      "Número de neuronas: 30\n",
      "-------------------------------------\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9271395033197608 +- 0.011509250769866157\n",
      "Validación: 0.9028514588859416 +- 0.0054196950367370175\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.9137931034482759, Especificidad: 0.8879310344827587\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: logistic\n",
      "Número de neuronas: (20, 20)\n",
      "-------------------------------------\n",
      "[0 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9507018573542179 +- 0.00707980729383657\n",
      "Validación: 0.9114230278023382 +- 0.01576168283274434\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.9224137931034483, Especificidad: 0.896551724137931\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: logistic\n",
      "Número de neuronas: (35, 35)\n",
      "-------------------------------------\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9606886516757762 +- 0.014938465543709659\n",
      "Validación: 0.9057250221043325 +- 0.0031786944373110053\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.9224137931034483, Especificidad: 0.896551724137931\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: logistic\n",
      "Número de neuronas: (35, 35, 20)\n",
      "-------------------------------------\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9621620630204321 +- 0.009863041181189934\n",
      "Validación: 0.9171333136850378 +- 0.01073105469498877\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.9051724137931034, Especificidad: 0.9224137931034483\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: tanh\n",
      "Número de neuronas: 20\n",
      "-------------------------------------\n",
      "[0 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9292548573664453 +- 0.0161526012137204\n",
      "Validación: 0.8942798899695451 +- 0.008841545363740655\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.8879310344827587, Especificidad: 0.896551724137931\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: tanh\n",
      "Número de neuronas: 30\n",
      "-------------------------------------\n",
      "[0 1 1 1 1 0 0 0 0 0]\n",
      "[1 0 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9400089260604281 +- 0.006221551339065431\n",
      "Validación: 0.8786349346694174 +- 0.016099626559608993\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.9051724137931034, Especificidad: 0.896551724137931\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: tanh\n",
      "Número de neuronas: (20, 20)\n",
      "-------------------------------------\n",
      "[0 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[0 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9499743222919189 +- 0.013443469967238662\n",
      "Validación: 0.9128475292268395 +- 0.024540719371842833\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.8793103448275862, Especificidad: 0.9396551724137931\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: tanh\n",
      "Número de neuronas: (35, 35)\n",
      "-------------------------------------\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1 0 1 1 1 0 0 0 0 1]\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9435273834415465 +- 0.03762283643515995\n",
      "Validación: 0.8629776991845958 +- 0.03855882567089205\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.9137931034482759, Especificidad: 0.896551724137931\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "Función de activación: tanh\n",
      "Número de neuronas: (35, 35, 20)\n",
      "-------------------------------------\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[0 0 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 0 0 1]\n",
      "-------------------------------------\n",
      "Entrenamiento: 0.9642651895870781 +- 0.011438505316166086\n",
      "Validación: 0.8899204244031829 +- 0.023173225880432212\n",
      "-------------------------------------\n",
      "Sensibilidad: 0.896551724137931, Especificidad: 0.8275862068965517\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "activation_functions = ['logistic', 'tanh']\n",
    "neurons_config = [20, 30, (20, 20), (35, 35), (35, 35, 20)]\n",
    "\n",
    "for activation in activation_functions:\n",
    "    for neurons in neurons_config:\n",
    "        \n",
    "        print('-------------------------------------')\n",
    "        print('Función de activación: {0}'.format(activation))\n",
    "        print('Número de neuronas: {0}'.format(neurons))\n",
    "        print('-------------------------------------')\n",
    "        \n",
    "        fold = 3\n",
    "        np.random.seed(12345)\n",
    "        efficiency_train = np.zeros(fold)\n",
    "        efficiency_validation = np.zeros(fold)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=fold, random_state=None, shuffle=False)\n",
    "\n",
    "        j = 0\n",
    "        for train, test in skf.split(features, target):\n",
    "            x_train = features[train, :]\n",
    "            y_train = target[train]\n",
    "            x_test = features[test, :]\n",
    "            y_test = target[test]\n",
    "\n",
    "            # ANN model\n",
    "            model = MLPClassifier(hidden_layer_sizes=neurons, activation=activation, max_iter=500)\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            # Realizamos predicciones\n",
    "            y_est_train = model.predict(x_train)\n",
    "            y_est_test = model.predict(x_test)\n",
    "            print(model.predict(local_test))\n",
    "\n",
    "            # Evaluamos las predicciones del modelo con los datos de test\n",
    "            efficiency_train[j] = np.mean(y_est_train == y_train)\n",
    "            efficiency_validation[j] = np.mean(y_est_test == y_test)\n",
    "            \n",
    "            # Medimos Sensibilidad y Especificidad\n",
    "            tp, fp, tn, fn = get_meassure(y_test, y_est_test)\n",
    "            sensitive = get_sensitive(tp, fn)\n",
    "            specificity = get_specificity(tn, fp)\n",
    "            \n",
    "            j += 1\n",
    "\n",
    "        print('-------------------------------------')\n",
    "        print('Entrenamiento: {0} +- {1}'.format(str(np.mean(efficiency_train)), str(np.std(efficiency_train))))\n",
    "        print('Validación: {0} +- {1}'.format(str(np.mean(efficiency_validation)), str(np.std(efficiency_validation))))\n",
    "        print('-------------------------------------')\n",
    "        print('Sensibilidad: {0}, Especificidad: {1}'.format(sensitive, specificity))\n",
    "        print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vemos que el modelo realiza predicciones más acertadas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
